{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Exercise Sheet 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for all exercises\n",
    "from nltk import corpus\n",
    "import re\n",
    "import random as r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Rewrite the following loop as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3),\n",
       " ('dog', 3),\n",
       " ('gave', 4),\n",
       " ('John', 4),\n",
       " ('the', 3),\n",
       " ('newspaper', 9)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper'] \n",
    "result = [] \n",
    "for word in sent: \n",
    "    word_len = (word, len(word)) \n",
    "    result.append(word_len) \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3),\n",
       " ('dog', 3),\n",
       " ('gave', 4),\n",
       " ('John', 4),\n",
       " ('the', 3),\n",
       " ('newspaper', 9)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ (w, len(w)) for w in sent ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append \"ay\", e.g. \"string\" $\\rightarrow$ \"ingstray\". If a word starts with a vowel, just add \"way\" to the end, e.g. \"idle\" $\\rightarrow$ \"idleway\". \n",
    "\n",
    "Write a function to convert a word to Pig Latin. Test it with the words \"pig\", \"cheers\", and \"omelet\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igpay\n",
      "eerschay\n",
      "omeletway\n"
     ]
    }
   ],
   "source": [
    "def pig_latin(word):\n",
    "    # returns tuple of (consonants?)(restOfWord)\n",
    "    pattern = r'^([^aeiou]*)(\\w*)'\n",
    "    pre_cons = re.findall(pattern, word)[0]\n",
    "\n",
    "    # has consonant cluster at start\n",
    "    if pre_cons[0]:\n",
    "        return pre_cons[1] + pre_cons[0] + 'ay'\n",
    "    # starts with vowel\n",
    "    else:\n",
    "        return pre_cons[1] + 'way'\n",
    "\n",
    "print(pig_latin('pig'))\n",
    "print(pig_latin('cheers'))\n",
    "print(pig_latin('omelet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Python's `random` module includes a function `choice()` which randomly chooses an item from a sequence, e.g. `choice('aehh ')` will produce one of four possible characters, with the letter \"h\" being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the `''.join()` function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: \"he  haha ee  heheeh eha\". Use `split()` and `join()` again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly generated string of length 500:\n",
      " ha hhaahehaha aae ahheae  h ee  ahahheae  eeahahhhhaahhe ah hehhhh  aeh a aah   e hhhaeh  h hhaeh hh hhaehh ee h ehhe eahhe h   aa eeha aaeh  hhhaahaeah hhaehahaaahahehhhhha ehah eaahh haeh hhh heah h    heeea h hh    h aheae eaea hhhaaaee h ahahh hhhh ehaeh   h  aahehahee h hehaeehee hheaehha hhhe ah h heahe haeehha hha heaeeheeh ahhaa aeaeeaaehhhhhah ehe hh hehe hhhae haehhehhh hhea aeaehaeheeehahhehe e aaehh aehehhhehaahhhehehahhehehh hhhhaehaheaaaaaahhhhahhehaehe eh    ah aaeae aah hha aah a\n",
      "\n",
      " Normalize the whitespaces\n",
      "ha hhaahehaha aae ahheae h ee ahahheae eeahahhhhaahhe ah hehhhh aeh a aah e hhhaeh h hhaeh hh hhaehh ee h ehhe eahhe h aa eeha aaeh hhhaahaeah hhaehahaaahahehhhhha ehah eaahh haeh hhh heah h heeea h hh h aheae eaea hhhaaaee h ahahh hhhh ehaeh h aahehahee h hehaeehee hheaehha hhhe ah h heahe haeehha hha heaeeheeh ahhaa aeaeeaaehhhhhah ehe hh hehe hhhae haehhehhh hhea aeaehaeheeehahhehe e aaehh aehehhhehaahhhehehahhehehh hhhhaehaheaaaaaahhhhahhehaehe eh ah aaeae aah hha aah a\n"
     ]
    }
   ],
   "source": [
    "def gen_random():\n",
    "    random_chars = [ r.choice('aehh ') for _ in range(500) ] \n",
    "    return ''.join(random_chars)\n",
    "\n",
    "generated = gen_random()\n",
    "print('Randomly generated string of length 500:\\n', generated)\n",
    "\n",
    "print('\\n Normalize the whitespaces')\n",
    "print(' '.join(generated.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define $\\mu_w$ to be the average number of letters per word, and $\\mu_s$ to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: $4.71 \\mu_w + 0.5 \\mu_s - 21.43$. Compute the ARI score for the \"lore\" and \"learned\" genre of the Brown Corpus. Make use of the fact that `nltk.corpus.brown.words()` produces a sequence of words, while `nltk.corpus.brown.sents()` produces a sequence of sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ARI score for category lore is 10.38404875845908\n",
      "The ARI score for category learned is 12.151973535006093\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ari(category):\n",
    "    words = corpus.brown.words(categories=category)\n",
    "    sents = corpus.brown.sents(categories=category)\n",
    "\n",
    "    # only take alphanumeric characters\n",
    "    words_sanitized = [w for w in words if re.search(r'^(\\w+)$', w)]\n",
    "\n",
    "    n_letters = sum(len(w) for w in words_sanitized)\n",
    "\n",
    "    # letter per word\n",
    "    yw = n_letters / len(words_sanitized)\n",
    "    # words per sentence\n",
    "    ys = len(words_sanitized) / len(sents)\n",
    "\n",
    "    ari_score = 4.71 * yw + 0.5 * ys - 21.43\n",
    "    print( f'The ARI score for category {category} is {ari_score}' )\n",
    "\n",
    "ari('lore')\n",
    "ari('learned')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Define a variable `silly` to contain the string: 'newly formed bland ideas are inexpressible in an infuriating way'. Now write code to perform the following tasks:\n",
    "\n",
    "a) Split `silly` into a list of strings, one per word, using Python's `split()` operation, and save this to a variable called `bland`.  \n",
    "b) Extract the second letter of each word in `silly` and join them into a string, to get 'eoldrnnnna'.  \n",
    "c) Combine the words in `bland` back into a single string, using `join()`. Make sure the words in the resulting string are separated with whitespace.  \n",
    "d) Print the words of `silly` in alphabetical order, one per line.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "bland = silly.split()\n",
    "print(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eoldrnnnna'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b)\n",
    "''.join([w[1] for w in bland])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are inexpressible in an infuriating way'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c)\n",
    "' '.join(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "# d)\n",
    "for w in sorted(bland):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Rewrite the following nested loop as a nested list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'tenacious', 'elocution',\n",
    "         'sequoia', 'tenacious', 'unidirectional']\n",
    "vsequences = set()\n",
    "for word in words:\n",
    "    vowels = []\n",
    "    for char in word:\n",
    "        if char in 'aeiou':\n",
    "            vowels.append(char)\n",
    "    vsequences.add(''.join(vowels))\n",
    "sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set( ''.join(re.findall(r'[aeiou]', w)) for w in words ))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bd1aae67afee37140d3228e7a33e02fcb0eebdfd37531e445d53af60db5a548"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
